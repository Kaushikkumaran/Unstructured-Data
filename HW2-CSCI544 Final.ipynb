{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeevithagc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeevithagc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import contractions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/jeevithagc/Documents/Knowledge/CSCI544\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"amazon_reviews_us_Jewelry_v1_00.tsv\", sep='\\t+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Reviews and Ratings\n",
    "data1 = data[['star_rating', 'review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star_rating</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             review_body\n",
       "star_rating             \n",
       "1                  19999\n",
       "2                  20000\n",
       "3                  19999\n",
       "4                  20000\n",
       "5                  19999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 20000 reviews randomly from each rating class\n",
    "data_sampled = data1.groupby('star_rating', group_keys=False).apply(lambda x: x.sample(20000))\n",
    "data_sampled.groupby('star_rating').agg({'review_body':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "Refer to https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook for more steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating    0\n",
       "review_body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eliminate missing values\n",
    "data_sampled = data_sampled.dropna().reset_index(drop=True)\n",
    "data_sampled.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>i did not like them at allbecause when i was r...</td>\n",
       "      <td>[i, did, not, like, them, at, allbecause, when...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>not impressed with this bead it is just a milk...</td>\n",
       "      <td>[not, impressed, with, this, bead, it, is, jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>warningthis locket is ridiculously small liter...</td>\n",
       "      <td>[warningthis, locket, is, ridiculously, small,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>to hard to open</td>\n",
       "      <td>[to, hard, to, open]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>chains were not like in description cheap look...</td>\n",
       "      <td>[chains, were, not, like, in, description, che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>5</td>\n",
       "      <td>i have bought maybe a dozen eternity rings on ...</td>\n",
       "      <td>[i, have, bought, maybe, a, dozen, eternity, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>5</td>\n",
       "      <td>very different and unique love them</td>\n",
       "      <td>[very, different, and, unique, love, them]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>5</td>\n",
       "      <td>simple yet elegant for the non girly girl manl...</td>\n",
       "      <td>[simple, yet, elegant, for, the, non, girly, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>5</td>\n",
       "      <td>it is an excellent article comply with the spe...</td>\n",
       "      <td>[it, is, an, excellent, article, comply, with,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>5</td>\n",
       "      <td>speedy delivery nice product for the price</td>\n",
       "      <td>[speedy, delivery, nice, product, for, the, pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90797 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       star_rating                                        review_body  \\\n",
       "0                1  i did not like them at allbecause when i was r...   \n",
       "1                1  not impressed with this bead it is just a milk...   \n",
       "2                1  warningthis locket is ridiculously small liter...   \n",
       "3                1                                    to hard to open   \n",
       "4                1  chains were not like in description cheap look...   \n",
       "...            ...                                                ...   \n",
       "99991            5  i have bought maybe a dozen eternity rings on ...   \n",
       "99992            5                very different and unique love them   \n",
       "99994            5  simple yet elegant for the non girly girl manl...   \n",
       "99995            5  it is an excellent article comply with the spe...   \n",
       "99996            5         speedy delivery nice product for the price   \n",
       "\n",
       "                                           review_tokens  \n",
       "0      [i, did, not, like, them, at, allbecause, when...  \n",
       "1      [not, impressed, with, this, bead, it, is, jus...  \n",
       "2      [warningthis, locket, is, ridiculously, small,...  \n",
       "3                                   [to, hard, to, open]  \n",
       "4      [chains, were, not, like, in, description, che...  \n",
       "...                                                  ...  \n",
       "99991  [i, have, bought, maybe, a, dozen, eternity, r...  \n",
       "99992         [very, different, and, unique, love, them]  \n",
       "99994  [simple, yet, elegant, for, the, non, girly, g...  \n",
       "99995  [it, is, an, excellent, article, comply, with,...  \n",
       "99996  [speedy, delivery, nice, product, for, the, pr...  \n",
       "\n",
       "[90797 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing NaN\n",
    "data_sampled = data_sampled.dropna()\n",
    "data_sampled = data_sampled.drop_duplicates()\n",
    "\n",
    "# lower case\n",
    "data_sampled['review_body'] = data_sampled['review_body'].str.lower()\n",
    "\n",
    "# remove the HTML and URLs from the reviews\n",
    "data_sampled['review_body'] = data_sampled['review_body'].map(lambda x: re.sub(r'http\\S+', '', x)) \n",
    "data_sampled['review_body'] = data_sampled['review_body'].map(lambda x: re.sub(r'<.*?>', '', x)) \n",
    "\n",
    "# remove non-alphabetical characters\n",
    "data_sampled['review_body'] = data_sampled['review_body'].str.replace('[^A-Za-z ]+', '')\n",
    "\n",
    "# remove whitespaces\n",
    "# data_sampled['review_body']=data_sampled['review_body'].str.strip()\n",
    "data_sampled['review_body']=data_sampled['review_body'].str.split()\n",
    "data_sampled['review_body']=data_sampled['review_body'].map(lambda x:' '.join(x))\n",
    "\n",
    "# perform contractions on the reviews\n",
    "data_sampled['review_body']=data_sampled['review_body'].map(lambda x:contractions.fix(x))\n",
    "\n",
    "# remove missing values and duplicates\n",
    "from nltk.tokenize import word_tokenize\n",
    "data_sampled = data_sampled.dropna().drop_duplicates()\n",
    "data_sampled['review_tokens'] = data_sampled['review_body'].apply(lambda text:word_tokenize(text))\n",
    "data_sampled = data_sampled.drop((data_sampled.loc[data_sampled['review_tokens'].str.len()<3]).index)\n",
    "data_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sampled_x = data_sampled.copy()\n",
    "# data_sampled_x['review_body_list'] = data_sampled_x['review_body'].str.split()\n",
    "# data_sampled_x['len_words'] = data_sampled_x['review_body_list'].map(lambda x:len(x))\n",
    "# data_sampled_x[data_sampled_x['len_words']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a)\n",
    "\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006666666666666667\n",
      "0.01\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "# (b) Training Word2Vec on dataset\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "cores = multiprocessing.cpu_count() # count number of cores\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "sent = [row.split() for row in data_sampled['review_body']]\n",
    "phrases = Phrases(sent)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "# data_sampled['sent'] = None\n",
    "# for i in range(len(sentences)):\n",
    "#   data_sampled['sent'][i] = sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('computers', 0.7979379296302795), ('laptop', 0.6640492677688599), ('laptop_computer', 0.6548868417739868)]\n",
      "0.3883018\n",
      "chair\n"
     ]
    }
   ],
   "source": [
    "# Eg 1\n",
    "print(wv.most_similar('computer', topn=3))\n",
    "\n",
    "# Eg 2\n",
    "print(wv.similarity('beautiful', 'horrible'))\n",
    "\n",
    "# Eg 3\n",
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'chair']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.53 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "w2v_model = Word2Vec(min_count=10, window=11, vector_size=300, workers=cores-1)\n",
    "# w2v_model.save(\"w2v_model.model\")\n",
    "# model = Word2Vec.load(\"w2v_model.model\")\n",
    "\n",
    "# Building the vocab tree\n",
    "t = time()\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "# model training\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "# print(w2v_model.get_latest_training_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('screen', 0.6647399067878723), ('website', 0.5683073401451111), ('site', 0.48910003900527954)]\n",
      "0.2534125\n",
      "fire\n"
     ]
    }
   ],
   "source": [
    "# Eg 1\n",
    "print(w2v_model.wv.most_similar('computer', topn=3))\n",
    "\n",
    "# Eg 2\n",
    "print(w2v_model.wv.similarity('beautiful', 'horrible'))\n",
    "\n",
    "# Eg 3\n",
    "print(w2v_model.wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'chair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = data_sampled['star_rating'].map(lambda x: x-1)\n",
    "X_input = data_sampled['review_tokens']\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_output, test_size=0.2, random_state=42)\n",
    "words = set(wv.index_to_key)\n",
    "X_train_vect = np.array([np.array([wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])\n",
    "X_test_vect = np.array([np.array([wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating average vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(300, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(300, dtype=float))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2806679511881824,0.9538881309686221,0.43371999255629307\n",
      "0.2727272727272727,0.012483745123537062,0.023874658045262374\n",
      "0.38245412844036697,0.1781517094017094,0.24307580174927115\n",
      "0.43112701252236135,0.06672203765227021,0.11555981778949892\n",
      "0.5916149068322981,0.5792034052903619,0.5853433707174682\n",
      "0.3917182543420963,0.35808980568730014,0.2803147281715587\n",
      "Mean accuracy on test input data and predicted labels: 0.3501514734232994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "clf = Perceptron(tol = 1e-8, random_state=0)\n",
    "clf.fit(X_train_vect_avg, y_train)\n",
    "y_train_pred = clf.predict(X_train_vect_avg)\n",
    "y_test_pred = clf.predict(X_test_vect_avg)\n",
    "\n",
    "# classification metrics\n",
    "cls_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "print(f\"{cls_report['0']['precision']},{cls_report['0']['recall']},{cls_report['0']['f1-score']}\")\n",
    "print(f\"{cls_report['1']['precision']},{cls_report['1']['recall']},{cls_report['1']['f1-score']}\")\n",
    "print(f\"{cls_report['2']['precision']},{cls_report['2']['recall']},{cls_report['2']['f1-score']}\")\n",
    "print(f\"{cls_report['3']['precision']},{cls_report['3']['recall']},{cls_report['3']['f1-score']}\")\n",
    "print(f\"{cls_report['4']['precision']},{cls_report['4']['recall']},{cls_report['4']['f1-score']}\")\n",
    "print(f\"{cls_report['macro avg']['precision']},{cls_report['macro avg']['recall']},{cls_report['macro avg']['f1-score']}\")\n",
    "\n",
    "print(f'Mean accuracy on test input data and predicted labels: {sum(y_test_pred == y_test)/y_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4969430645777608,0.70949263502455,0.584494382022472\n",
      "0.3881158330199323,0.27093725387240747,0.31910946196660483\n",
      "0.39165009940357853,0.366365568544102,0.378586135895676\n",
      "0.44811504080839487,0.3207232267037552,0.37386511024643315\n",
      "0.6005271986580398,0.7534576067348165,0.6683557807707694\n",
      "0.46507024729354124,0.48419525817592624,0.4648821741803911\n",
      "Mean accuracy on test input data and predicted labels: 0.4774779735682819\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svm_clf = svm.LinearSVC(random_state=0)\n",
    "svm_clf.fit(X_train_vect_avg, y_train)\n",
    "y_test_pred_svm = svm_clf.predict(X_test_vect_avg)\n",
    "\n",
    "# classification metrics\n",
    "cls_report = classification_report(y_test, y_test_pred_svm, output_dict=True)\n",
    "print(f\"{cls_report['0']['precision']},{cls_report['0']['recall']},{cls_report['0']['f1-score']}\")\n",
    "print(f\"{cls_report['1']['precision']},{cls_report['1']['recall']},{cls_report['1']['f1-score']}\")\n",
    "print(f\"{cls_report['2']['precision']},{cls_report['2']['recall']},{cls_report['2']['f1-score']}\")\n",
    "print(f\"{cls_report['3']['precision']},{cls_report['3']['recall']},{cls_report['3']['f1-score']}\")\n",
    "print(f\"{cls_report['4']['precision']},{cls_report['4']['recall']},{cls_report['4']['f1-score']}\")\n",
    "print(f\"{cls_report['macro avg']['precision']},{cls_report['macro avg']['recall']},{cls_report['macro avg']['f1-score']}\")\n",
    "\n",
    "print(f'Mean accuracy on test input data and predicted labels: {sum(y_test_pred_svm == y_test)/y_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "X_train_tensor = torch.Tensor(X_train_vect_avg)\n",
    "\n",
    "class review_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.labels = torch.LongTensor([i for i in y])\n",
    "        self.texts = torch.Tensor(X)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = np.array(self.labels[idx])\n",
    "\n",
    "        return batch_texts, batch_y\n",
    "    \n",
    "train_data, test_data = review_dataset(X_train_vect_avg, y_train), review_dataset(X_test_vect_avg, y_test)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_mod(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation_func = \"relu\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_fc = nn.Linear(input_dim, 50)\n",
    "        self.hidden_fc = nn.Linear(50, 10)\n",
    "        self.output_fc = nn.Linear(10, output_dim)\n",
    "        self.activation_function = activation_func\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        if self.activation_function == \"relu\":\n",
    "            h_1 = F.relu(self.input_fc(x))\n",
    "            h_2 = F.relu(self.hidden_fc(h_1))\n",
    "        if self.activation_function == \"leaky_relu\":\n",
    "            h_1 = F.leaky_relu(self.input_fc(x))\n",
    "            h_2 = F.leaky_relu(self.hidden_fc(h_1))\n",
    "        y_pred = self.output_fc(h_2)\n",
    "        return y_pred\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.input_fc = nn.Linear(input_dim, 50)\n",
    "#         self.hidden_fc = nn.Linear(50, 10)\n",
    "#         self.output_fc = nn.Linear(10, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         # x = [batch size, height, width]\n",
    "\n",
    "#         batch_size = x.shape[0]\n",
    "\n",
    "#         x = x.view(batch_size, -1)\n",
    "\n",
    "#         # x = [batch size, height * width]\n",
    "\n",
    "#         h_1 = F.relu(self.input_fc(x))\n",
    "\n",
    "#         # h_1 = [batch size, 250]\n",
    "\n",
    "#         h_2 = F.relu(self.hidden_fc(h_1))\n",
    "\n",
    "#         # h_2 = [batch size, 100]\n",
    "\n",
    "#         y_pred = self.output_fc(h_2)\n",
    "\n",
    "#         # y_pred = [batch size, output dim]\n",
    "\n",
    "#         return y_pred, h_2\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,xshape):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(xshape, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 5)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "       \n",
    "        x = F.relu(self.fc1(x))\n",
    "      \n",
    "        x = self.dropout(x)\n",
    "       \n",
    "        x = F.relu(self.fc2(x))\n",
    "       \n",
    "        x = self.dropout(x)\n",
    "     \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(300).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_accuracy(y_pred, y):\n",
    "#     top_pred = y_pred.argmax(1, keepdim=True)\n",
    "#     correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "#     acc = correct.float() / y.shape[0]\n",
    "#     return acc\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device, n_epochs):\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"epoch\",epoch)\n",
    "        epoch_loss = 0.0\n",
    "        # epoch_acc = 0\n",
    "        model.train()\n",
    "\n",
    "        for x, y in iterator:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            # acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()*x.size(0)\n",
    "            # epoch_loss += loss.item()\n",
    "            # epoch_acc += acc.item()\n",
    "        \n",
    "        epoch_loss = epoch_loss/len(iterator.dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "                epoch+1, \n",
    "                epoch_loss,\n",
    "                ))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch: 1 \tTraining Loss: 1.611035\n",
      "epoch 1\n",
      "Epoch: 2 \tTraining Loss: 1.606735\n",
      "epoch 2\n",
      "Epoch: 3 \tTraining Loss: 1.603359\n",
      "epoch 3\n",
      "Epoch: 4 \tTraining Loss: 1.596334\n",
      "epoch 4\n",
      "Epoch: 5 \tTraining Loss: 1.577726\n",
      "epoch 5\n",
      "Epoch: 6 \tTraining Loss: 1.529847\n",
      "epoch 6\n",
      "Epoch: 7 \tTraining Loss: 1.458246\n",
      "epoch 7\n",
      "Epoch: 8 \tTraining Loss: 1.406392\n",
      "epoch 8\n",
      "Epoch: 9 \tTraining Loss: 1.373051\n",
      "epoch 9\n",
      "Epoch: 10 \tTraining Loss: 1.349990\n",
      "epoch 10\n",
      "Epoch: 11 \tTraining Loss: 1.333961\n",
      "epoch 11\n",
      "Epoch: 12 \tTraining Loss: 1.321677\n",
      "epoch 12\n",
      "Epoch: 13 \tTraining Loss: 1.313083\n",
      "epoch 13\n",
      "Epoch: 14 \tTraining Loss: 1.303674\n",
      "epoch 14\n",
      "Epoch: 15 \tTraining Loss: 1.298315\n",
      "epoch 15\n",
      "Epoch: 16 \tTraining Loss: 1.291237\n",
      "epoch 16\n",
      "Epoch: 17 \tTraining Loss: 1.285353\n",
      "epoch 17\n",
      "Epoch: 18 \tTraining Loss: 1.280743\n",
      "epoch 18\n",
      "Epoch: 19 \tTraining Loss: 1.275397\n",
      "epoch 19\n",
      "Epoch: 20 \tTraining Loss: 1.270757\n",
      "epoch 20\n",
      "Epoch: 21 \tTraining Loss: 1.268415\n",
      "epoch 21\n",
      "Epoch: 22 \tTraining Loss: 1.264425\n",
      "epoch 22\n",
      "Epoch: 23 \tTraining Loss: 1.260338\n",
      "epoch 23\n",
      "Epoch: 24 \tTraining Loss: 1.257376\n",
      "epoch 24\n",
      "Epoch: 25 \tTraining Loss: 1.254094\n",
      "epoch 25\n",
      "Epoch: 26 \tTraining Loss: 1.252495\n",
      "epoch 26\n",
      "Epoch: 27 \tTraining Loss: 1.249007\n",
      "epoch 27\n",
      "Epoch: 28 \tTraining Loss: 1.246300\n",
      "epoch 28\n",
      "Epoch: 29 \tTraining Loss: 1.245579\n",
      "epoch 29\n",
      "Epoch: 30 \tTraining Loss: 1.243163\n",
      "epoch 30\n",
      "Epoch: 31 \tTraining Loss: 1.239116\n",
      "epoch 31\n",
      "Epoch: 32 \tTraining Loss: 1.238789\n",
      "epoch 32\n",
      "Epoch: 33 \tTraining Loss: 1.236717\n",
      "epoch 33\n",
      "Epoch: 34 \tTraining Loss: 1.234630\n",
      "epoch 34\n",
      "Epoch: 35 \tTraining Loss: 1.233691\n",
      "epoch 35\n",
      "Epoch: 36 \tTraining Loss: 1.233188\n",
      "epoch 36\n",
      "Epoch: 37 \tTraining Loss: 1.231145\n",
      "epoch 37\n",
      "Epoch: 38 \tTraining Loss: 1.228730\n",
      "epoch 38\n",
      "Epoch: 39 \tTraining Loss: 1.228610\n",
      "epoch 39\n",
      "Epoch: 40 \tTraining Loss: 1.226287\n",
      "epoch 40\n",
      "Epoch: 41 \tTraining Loss: 1.224100\n",
      "epoch 41\n",
      "Epoch: 42 \tTraining Loss: 1.222560\n",
      "epoch 42\n",
      "Epoch: 43 \tTraining Loss: 1.221668\n",
      "epoch 43\n",
      "Epoch: 44 \tTraining Loss: 1.222679\n",
      "epoch 44\n",
      "Epoch: 45 \tTraining Loss: 1.219254\n",
      "epoch 45\n",
      "Epoch: 46 \tTraining Loss: 1.219660\n",
      "epoch 46\n",
      "Epoch: 47 \tTraining Loss: 1.218089\n",
      "epoch 47\n",
      "Epoch: 48 \tTraining Loss: 1.215071\n",
      "epoch 48\n",
      "Epoch: 49 \tTraining Loss: 1.217143\n",
      "epoch 49\n",
      "Epoch: 50 \tTraining Loss: 1.216312\n"
     ]
    }
   ],
   "source": [
    "mlp_model = train(model,train_dataloader, optimizer, criterion, device, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "for i, batch in enumerate(test_dataloader):   \n",
    "    outputs = model(batch[0])\n",
    "    _, predicted = torch.max(outputs.data, 1) \n",
    "    prediction_list.append(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.6182869732856\n"
     ]
    }
   ],
   "source": [
    "# predictions = predict(mlp_model, test_dataloader)\n",
    "predictions = np.array(prediction_list)\n",
    "tp=0\n",
    "tp += (predictions == y_test).sum()\n",
    "accuracy = 100 * tp / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Combination : relu 100\n",
      "epoch 0\n",
      "Epoch: 1 \tTraining Loss: 1.291615\n",
      "epoch 1\n",
      "Epoch: 2 \tTraining Loss: 1.192348\n",
      "epoch 2\n",
      "Epoch: 3 \tTraining Loss: 1.172078\n",
      "epoch 3\n",
      "Epoch: 4 \tTraining Loss: 1.160785\n",
      "epoch 4\n",
      "Epoch: 5 \tTraining Loss: 1.152192\n",
      "epoch 5\n",
      "Epoch: 6 \tTraining Loss: 1.144928\n",
      "epoch 6\n",
      "Epoch: 7 \tTraining Loss: 1.138464\n",
      "epoch 7\n",
      "Epoch: 8 \tTraining Loss: 1.132707\n",
      "epoch 8\n",
      "Epoch: 9 \tTraining Loss: 1.127519\n",
      "epoch 9\n",
      "Epoch: 10 \tTraining Loss: 1.122516\n",
      "epoch 10\n",
      "Epoch: 11 \tTraining Loss: 1.117953\n",
      "epoch 11\n",
      "Epoch: 12 \tTraining Loss: 1.113587\n",
      "epoch 12\n",
      "Epoch: 13 \tTraining Loss: 1.109814\n",
      "epoch 13\n",
      "Epoch: 14 \tTraining Loss: 1.105957\n",
      "epoch 14\n",
      "Epoch: 15 \tTraining Loss: 1.102448\n",
      "epoch 15\n",
      "Epoch: 16 \tTraining Loss: 1.098978\n",
      "epoch 16\n",
      "Epoch: 17 \tTraining Loss: 1.095686\n",
      "epoch 17\n",
      "Epoch: 18 \tTraining Loss: 1.092565\n",
      "epoch 18\n",
      "Epoch: 19 \tTraining Loss: 1.089473\n",
      "epoch 19\n",
      "Epoch: 20 \tTraining Loss: 1.086778\n",
      "epoch 20\n",
      "Epoch: 21 \tTraining Loss: 1.083965\n",
      "epoch 21\n",
      "Epoch: 22 \tTraining Loss: 1.081440\n",
      "epoch 22\n",
      "Epoch: 23 \tTraining Loss: 1.078919\n",
      "epoch 23\n",
      "Epoch: 24 \tTraining Loss: 1.076490\n",
      "epoch 24\n",
      "Epoch: 25 \tTraining Loss: 1.074245\n",
      "epoch 25\n",
      "Epoch: 26 \tTraining Loss: 1.072059\n",
      "epoch 26\n",
      "Epoch: 27 \tTraining Loss: 1.069717\n",
      "epoch 27\n",
      "Epoch: 28 \tTraining Loss: 1.067527\n",
      "epoch 28\n",
      "Epoch: 29 \tTraining Loss: 1.065387\n",
      "epoch 29\n",
      "Epoch: 30 \tTraining Loss: 1.063284\n",
      "epoch 30\n",
      "Epoch: 31 \tTraining Loss: 1.061305\n",
      "epoch 31\n",
      "Epoch: 32 \tTraining Loss: 1.059325\n",
      "epoch 32\n",
      "Epoch: 33 \tTraining Loss: 1.057474\n",
      "epoch 33\n",
      "Epoch: 34 \tTraining Loss: 1.055671\n",
      "epoch 34\n",
      "Epoch: 35 \tTraining Loss: 1.053733\n",
      "epoch 35\n",
      "Epoch: 36 \tTraining Loss: 1.051914\n",
      "epoch 36\n",
      "Epoch: 37 \tTraining Loss: 1.050266\n",
      "epoch 37\n",
      "Epoch: 38 \tTraining Loss: 1.048652\n",
      "epoch 38\n",
      "Epoch: 39 \tTraining Loss: 1.047248\n",
      "epoch 39\n",
      "Epoch: 40 \tTraining Loss: 1.045646\n",
      "epoch 40\n",
      "Epoch: 41 \tTraining Loss: 1.044174\n",
      "epoch 41\n",
      "Epoch: 42 \tTraining Loss: 1.042676\n",
      "epoch 42\n",
      "Epoch: 43 \tTraining Loss: 1.041320\n",
      "epoch 43\n",
      "Epoch: 44 \tTraining Loss: 1.039857\n",
      "epoch 44\n",
      "Epoch: 45 \tTraining Loss: 1.038540\n",
      "epoch 45\n",
      "Epoch: 46 \tTraining Loss: 1.037115\n",
      "epoch 46\n",
      "Epoch: 47 \tTraining Loss: 1.036025\n",
      "epoch 47\n",
      "Epoch: 48 \tTraining Loss: 1.034687\n",
      "epoch 48\n",
      "Epoch: 49 \tTraining Loss: 1.033511\n",
      "epoch 49\n",
      "Epoch: 50 \tTraining Loss: 1.032216\n",
      "epoch 50\n",
      "Epoch: 51 \tTraining Loss: 1.030951\n",
      "epoch 51\n",
      "Epoch: 52 \tTraining Loss: 1.029989\n",
      "epoch 52\n",
      "Epoch: 53 \tTraining Loss: 1.028829\n",
      "epoch 53\n",
      "Epoch: 54 \tTraining Loss: 1.027522\n",
      "epoch 54\n",
      "Epoch: 55 \tTraining Loss: 1.026612\n",
      "epoch 55\n",
      "Epoch: 56 \tTraining Loss: 1.025578\n",
      "epoch 56\n",
      "Epoch: 57 \tTraining Loss: 1.024569\n",
      "epoch 57\n",
      "Epoch: 58 \tTraining Loss: 1.023552\n",
      "epoch 58\n",
      "Epoch: 59 \tTraining Loss: 1.022470\n",
      "epoch 59\n",
      "Epoch: 60 \tTraining Loss: 1.021449\n",
      "epoch 60\n",
      "Epoch: 61 \tTraining Loss: 1.020493\n",
      "epoch 61\n",
      "Epoch: 62 \tTraining Loss: 1.019393\n",
      "epoch 62\n",
      "Epoch: 63 \tTraining Loss: 1.018520\n",
      "epoch 63\n",
      "Epoch: 64 \tTraining Loss: 1.017705\n",
      "epoch 64\n",
      "Epoch: 65 \tTraining Loss: 1.016909\n",
      "epoch 65\n",
      "Epoch: 66 \tTraining Loss: 1.016012\n",
      "epoch 66\n",
      "Epoch: 67 \tTraining Loss: 1.015218\n",
      "epoch 67\n",
      "Epoch: 68 \tTraining Loss: 1.014122\n",
      "epoch 68\n",
      "Epoch: 69 \tTraining Loss: 1.013483\n",
      "epoch 69\n",
      "Epoch: 70 \tTraining Loss: 1.012646\n",
      "epoch 70\n",
      "Epoch: 71 \tTraining Loss: 1.012023\n",
      "epoch 71\n",
      "Epoch: 72 \tTraining Loss: 1.011271\n",
      "epoch 72\n",
      "Epoch: 73 \tTraining Loss: 1.010492\n",
      "epoch 73\n",
      "Epoch: 74 \tTraining Loss: 1.009695\n",
      "epoch 74\n",
      "Epoch: 75 \tTraining Loss: 1.009030\n",
      "epoch 75\n",
      "Epoch: 76 \tTraining Loss: 1.008377\n",
      "epoch 76\n",
      "Epoch: 77 \tTraining Loss: 1.007549\n",
      "epoch 77\n",
      "Epoch: 78 \tTraining Loss: 1.006887\n",
      "epoch 78\n",
      "Epoch: 79 \tTraining Loss: 1.006098\n",
      "epoch 79\n",
      "Epoch: 80 \tTraining Loss: 1.005458\n",
      "epoch 80\n",
      "Epoch: 81 \tTraining Loss: 1.004558\n",
      "epoch 81\n",
      "Epoch: 82 \tTraining Loss: 1.003981\n",
      "epoch 82\n",
      "Epoch: 83 \tTraining Loss: 1.003345\n",
      "epoch 83\n",
      "Epoch: 84 \tTraining Loss: 1.002679\n",
      "epoch 84\n",
      "Epoch: 85 \tTraining Loss: 1.002074\n",
      "epoch 85\n",
      "Epoch: 86 \tTraining Loss: 1.001429\n",
      "epoch 86\n",
      "Epoch: 87 \tTraining Loss: 1.000918\n",
      "epoch 87\n",
      "Epoch: 88 \tTraining Loss: 1.000051\n",
      "epoch 88\n",
      "Epoch: 89 \tTraining Loss: 0.999328\n",
      "epoch 89\n",
      "Epoch: 90 \tTraining Loss: 0.998744\n",
      "epoch 90\n",
      "Epoch: 91 \tTraining Loss: 0.998062\n",
      "epoch 91\n",
      "Epoch: 92 \tTraining Loss: 0.997431\n",
      "epoch 92\n",
      "Epoch: 93 \tTraining Loss: 0.996744\n",
      "epoch 93\n",
      "Epoch: 94 \tTraining Loss: 0.996370\n",
      "epoch 94\n",
      "Epoch: 95 \tTraining Loss: 0.995842\n",
      "epoch 95\n",
      "Epoch: 96 \tTraining Loss: 0.995144\n",
      "epoch 96\n",
      "Epoch: 97 \tTraining Loss: 0.994569\n",
      "epoch 97\n",
      "Epoch: 98 \tTraining Loss: 0.994033\n",
      "epoch 98\n",
      "Epoch: 99 \tTraining Loss: 0.993629\n",
      "epoch 99\n",
      "Epoch: 100 \tTraining Loss: 0.992982\n",
      "Running for Combination : leaky_relu 100\n",
      "epoch 0\n",
      "Epoch: 1 \tTraining Loss: 1.289803\n",
      "epoch 1\n",
      "Epoch: 2 \tTraining Loss: 1.186359\n",
      "epoch 2\n",
      "Epoch: 3 \tTraining Loss: 1.168100\n",
      "epoch 3\n",
      "Epoch: 4 \tTraining Loss: 1.156754\n",
      "epoch 4\n",
      "Epoch: 5 \tTraining Loss: 1.147430\n",
      "epoch 5\n",
      "Epoch: 6 \tTraining Loss: 1.139436\n",
      "epoch 6\n",
      "Epoch: 7 \tTraining Loss: 1.132479\n",
      "epoch 7\n",
      "Epoch: 8 \tTraining Loss: 1.126270\n",
      "epoch 8\n",
      "Epoch: 9 \tTraining Loss: 1.120837\n",
      "epoch 9\n",
      "Epoch: 10 \tTraining Loss: 1.115812\n",
      "epoch 10\n",
      "Epoch: 11 \tTraining Loss: 1.111146\n",
      "epoch 11\n",
      "Epoch: 12 \tTraining Loss: 1.106827\n",
      "epoch 12\n",
      "Epoch: 13 \tTraining Loss: 1.102762\n",
      "epoch 13\n",
      "Epoch: 14 \tTraining Loss: 1.099083\n",
      "epoch 14\n",
      "Epoch: 15 \tTraining Loss: 1.095438\n",
      "epoch 15\n",
      "Epoch: 16 \tTraining Loss: 1.091925\n",
      "epoch 16\n",
      "Epoch: 17 \tTraining Loss: 1.088567\n",
      "epoch 17\n",
      "Epoch: 18 \tTraining Loss: 1.085116\n",
      "epoch 18\n",
      "Epoch: 19 \tTraining Loss: 1.082192\n",
      "epoch 19\n",
      "Epoch: 20 \tTraining Loss: 1.079322\n",
      "epoch 20\n",
      "Epoch: 21 \tTraining Loss: 1.076504\n",
      "epoch 21\n",
      "Epoch: 22 \tTraining Loss: 1.073746\n",
      "epoch 22\n",
      "Epoch: 23 \tTraining Loss: 1.071289\n",
      "epoch 23\n",
      "Epoch: 24 \tTraining Loss: 1.068749\n",
      "epoch 24\n",
      "Epoch: 25 \tTraining Loss: 1.066206\n",
      "epoch 25\n",
      "Epoch: 26 \tTraining Loss: 1.063925\n",
      "epoch 26\n",
      "Epoch: 27 \tTraining Loss: 1.061596\n",
      "epoch 27\n",
      "Epoch: 28 \tTraining Loss: 1.059406\n",
      "epoch 28\n",
      "Epoch: 29 \tTraining Loss: 1.057431\n",
      "epoch 29\n",
      "Epoch: 30 \tTraining Loss: 1.055288\n",
      "epoch 30\n",
      "Epoch: 31 \tTraining Loss: 1.053120\n",
      "epoch 31\n",
      "Epoch: 32 \tTraining Loss: 1.050908\n",
      "epoch 32\n",
      "Epoch: 33 \tTraining Loss: 1.049188\n",
      "epoch 33\n",
      "Epoch: 34 \tTraining Loss: 1.047334\n",
      "epoch 34\n",
      "Epoch: 35 \tTraining Loss: 1.045510\n",
      "epoch 35\n",
      "Epoch: 36 \tTraining Loss: 1.043949\n",
      "epoch 36\n",
      "Epoch: 37 \tTraining Loss: 1.041974\n",
      "epoch 37\n",
      "Epoch: 38 \tTraining Loss: 1.040394\n",
      "epoch 38\n",
      "Epoch: 39 \tTraining Loss: 1.038621\n",
      "epoch 39\n",
      "Epoch: 40 \tTraining Loss: 1.036977\n",
      "epoch 40\n",
      "Epoch: 41 \tTraining Loss: 1.035639\n",
      "epoch 41\n",
      "Epoch: 42 \tTraining Loss: 1.034061\n",
      "epoch 42\n",
      "Epoch: 43 \tTraining Loss: 1.032697\n",
      "epoch 43\n",
      "Epoch: 44 \tTraining Loss: 1.031198\n",
      "epoch 44\n",
      "Epoch: 45 \tTraining Loss: 1.029595\n",
      "epoch 45\n",
      "Epoch: 46 \tTraining Loss: 1.028516\n",
      "epoch 46\n",
      "Epoch: 47 \tTraining Loss: 1.027010\n",
      "epoch 47\n",
      "Epoch: 48 \tTraining Loss: 1.025669\n",
      "epoch 48\n",
      "Epoch: 49 \tTraining Loss: 1.024375\n",
      "epoch 49\n",
      "Epoch: 50 \tTraining Loss: 1.023084\n",
      "epoch 50\n",
      "Epoch: 51 \tTraining Loss: 1.021846\n",
      "epoch 51\n",
      "Epoch: 52 \tTraining Loss: 1.020634\n",
      "epoch 52\n",
      "Epoch: 53 \tTraining Loss: 1.019461\n",
      "epoch 53\n",
      "Epoch: 54 \tTraining Loss: 1.018153\n",
      "epoch 54\n",
      "Epoch: 55 \tTraining Loss: 1.017013\n",
      "epoch 55\n",
      "Epoch: 56 \tTraining Loss: 1.016192\n",
      "epoch 56\n",
      "Epoch: 57 \tTraining Loss: 1.014948\n",
      "epoch 57\n",
      "Epoch: 58 \tTraining Loss: 1.013942\n",
      "epoch 58\n",
      "Epoch: 59 \tTraining Loss: 1.012986\n",
      "epoch 59\n",
      "Epoch: 60 \tTraining Loss: 1.012050\n",
      "epoch 60\n",
      "Epoch: 61 \tTraining Loss: 1.011073\n",
      "epoch 61\n",
      "Epoch: 62 \tTraining Loss: 1.010043\n",
      "epoch 62\n",
      "Epoch: 63 \tTraining Loss: 1.008994\n",
      "epoch 63\n",
      "Epoch: 64 \tTraining Loss: 1.008211\n",
      "epoch 64\n",
      "Epoch: 65 \tTraining Loss: 1.007061\n",
      "epoch 65\n",
      "Epoch: 66 \tTraining Loss: 1.006434\n",
      "epoch 66\n",
      "Epoch: 67 \tTraining Loss: 1.005119\n",
      "epoch 67\n",
      "Epoch: 68 \tTraining Loss: 1.004432\n",
      "epoch 68\n",
      "Epoch: 69 \tTraining Loss: 1.003526\n",
      "epoch 69\n",
      "Epoch: 70 \tTraining Loss: 1.002529\n",
      "epoch 70\n",
      "Epoch: 71 \tTraining Loss: 1.001515\n",
      "epoch 71\n",
      "Epoch: 72 \tTraining Loss: 1.000633\n",
      "epoch 72\n",
      "Epoch: 73 \tTraining Loss: 0.999698\n",
      "epoch 73\n",
      "Epoch: 74 \tTraining Loss: 0.998981\n",
      "epoch 74\n",
      "Epoch: 75 \tTraining Loss: 0.998010\n",
      "epoch 75\n",
      "Epoch: 76 \tTraining Loss: 0.997323\n",
      "epoch 76\n",
      "Epoch: 77 \tTraining Loss: 0.996462\n",
      "epoch 77\n",
      "Epoch: 78 \tTraining Loss: 0.995895\n",
      "epoch 78\n",
      "Epoch: 79 \tTraining Loss: 0.995057\n",
      "epoch 79\n",
      "Epoch: 80 \tTraining Loss: 0.994265\n",
      "epoch 80\n",
      "Epoch: 81 \tTraining Loss: 0.993732\n",
      "epoch 81\n",
      "Epoch: 82 \tTraining Loss: 0.992939\n",
      "epoch 82\n",
      "Epoch: 83 \tTraining Loss: 0.992380\n",
      "epoch 83\n",
      "Epoch: 84 \tTraining Loss: 0.991294\n",
      "epoch 84\n",
      "Epoch: 85 \tTraining Loss: 0.990503\n",
      "epoch 85\n",
      "Epoch: 86 \tTraining Loss: 0.989836\n",
      "epoch 86\n",
      "Epoch: 87 \tTraining Loss: 0.989209\n",
      "epoch 87\n",
      "Epoch: 88 \tTraining Loss: 0.988201\n",
      "epoch 88\n",
      "Epoch: 89 \tTraining Loss: 0.987463\n",
      "epoch 89\n",
      "Epoch: 90 \tTraining Loss: 0.986782\n",
      "epoch 90\n",
      "Epoch: 91 \tTraining Loss: 0.986120\n",
      "epoch 91\n",
      "Epoch: 92 \tTraining Loss: 0.985409\n",
      "epoch 92\n",
      "Epoch: 93 \tTraining Loss: 0.984815\n",
      "epoch 93\n",
      "Epoch: 94 \tTraining Loss: 0.984193\n",
      "epoch 94\n",
      "Epoch: 95 \tTraining Loss: 0.983539\n",
      "epoch 95\n",
      "Epoch: 96 \tTraining Loss: 0.983109\n",
      "epoch 96\n",
      "Epoch: 97 \tTraining Loss: 0.982417\n",
      "epoch 97\n",
      "Epoch: 98 \tTraining Loss: 0.981784\n",
      "epoch 98\n",
      "Epoch: 99 \tTraining Loss: 0.981078\n",
      "epoch 99\n",
      "Epoch: 100 \tTraining Loss: 0.980511\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = [100]\n",
    "activation_funcs = [\"relu\", \"leaky_relu\"]\n",
    "from itertools import product\n",
    "output_df = pd.DataFrame(product(activation_funcs, N_EPOCHS), columns = ['Activation_Function', 'N_Epochs'])\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "accuracies = []\n",
    "for activation_func in activation_funcs:\n",
    "    for i, epochs in enumerate(N_EPOCHS):\n",
    "        print(\"Running for Combination :\", activation_func, epochs)\n",
    "        INPUT_DIM = 300\n",
    "        OUTPUT_DIM = 5\n",
    "\n",
    "        model = MLP_mod(INPUT_DIM, OUTPUT_DIM, activation_func)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "        model_updated = train(model, train_dataloader, optimizer, criterion, device, epochs)\n",
    "            \n",
    "        prediction_list = []\n",
    "        for i, batch in enumerate(test_dataloader):   \n",
    "            outputs = model_updated(batch[0])\n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            prediction_list.append(predicted)    \n",
    "            \n",
    "        predictions = np.array(prediction_list)\n",
    "        tp=0\n",
    "        tp += (predictions == y_test).sum()\n",
    "        accuracy = 100 * tp / len(predictions)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "output_df['Test_Accuracy'] = accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation_Function</th>\n",
       "      <th>N_Epochs</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>48.256679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>100</td>\n",
       "      <td>48.477004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation_Function  N_Epochs  Test_Accuracy\n",
       "0                relu       100      48.256679\n",
       "1          leaky_relu       100      48.477004"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain best params\n",
    "# output_df.loc[output_df['Test_Accuracy'].idxmax()]\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect_concat = []\n",
    "for i in X_train_vect:\n",
    "    subset_list = []\n",
    "    for v in range(10):\n",
    "        try:\n",
    "            if i[v].size:\n",
    "                subset_list.extend(i[v])\n",
    "        except:\n",
    "            subset_list.extend(np.zeros(300, dtype=float))\n",
    "    X_train_vect_concat.append(subset_list)\n",
    "X_train_vect_concat = np.array(X_train_vect_concat) \n",
    "        \n",
    "X_test_vect_concat = []\n",
    "for i in X_test_vect:\n",
    "    subset_list = []\n",
    "    for v in range(10):\n",
    "        try:\n",
    "            if i[v].size:\n",
    "                subset_list.extend(i[v])\n",
    "        except:\n",
    "            subset_list.extend(np.zeros(300, dtype=float))\n",
    "    X_test_vect_concat.append(subset_list)\n",
    "X_test_vect_concat = np.array(X_test_vect_concat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18155\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_vect_concat))\n",
    "print(len(X_test_vect_concat[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Combination : relu 100\n",
      "epoch 0\n",
      "Epoch: 1 \tTraining Loss: 1.325984\n",
      "epoch 1\n",
      "Epoch: 2 \tTraining Loss: 1.243667\n",
      "epoch 2\n",
      "Epoch: 3 \tTraining Loss: 1.203100\n",
      "epoch 3\n",
      "Epoch: 4 \tTraining Loss: 1.162756\n",
      "epoch 4\n",
      "Epoch: 5 \tTraining Loss: 1.122003\n",
      "epoch 5\n",
      "Epoch: 6 \tTraining Loss: 1.081444\n",
      "epoch 6\n",
      "Epoch: 7 \tTraining Loss: 1.040732\n",
      "epoch 7\n",
      "Epoch: 8 \tTraining Loss: 1.000380\n",
      "epoch 8\n",
      "Epoch: 9 \tTraining Loss: 0.961321\n",
      "epoch 9\n",
      "Epoch: 10 \tTraining Loss: 0.922671\n",
      "epoch 10\n",
      "Epoch: 11 \tTraining Loss: 0.885373\n",
      "epoch 11\n",
      "Epoch: 12 \tTraining Loss: 0.849837\n",
      "epoch 12\n",
      "Epoch: 13 \tTraining Loss: 0.817757\n",
      "epoch 13\n",
      "Epoch: 14 \tTraining Loss: 0.785639\n",
      "epoch 14\n",
      "Epoch: 15 \tTraining Loss: 0.755246\n",
      "epoch 15\n",
      "Epoch: 16 \tTraining Loss: 0.726884\n",
      "epoch 16\n",
      "Epoch: 17 \tTraining Loss: 0.702159\n",
      "epoch 17\n",
      "Epoch: 18 \tTraining Loss: 0.679145\n",
      "epoch 18\n",
      "Epoch: 19 \tTraining Loss: 0.657398\n",
      "epoch 19\n",
      "Epoch: 20 \tTraining Loss: 0.635994\n",
      "epoch 20\n",
      "Epoch: 21 \tTraining Loss: 0.618694\n",
      "epoch 21\n",
      "Epoch: 22 \tTraining Loss: 0.608478\n",
      "epoch 22\n",
      "Epoch: 23 \tTraining Loss: 0.593830\n",
      "epoch 23\n",
      "Epoch: 24 \tTraining Loss: 0.581350\n",
      "epoch 24\n",
      "Epoch: 25 \tTraining Loss: 0.570443\n",
      "epoch 25\n",
      "Epoch: 26 \tTraining Loss: 0.553896\n",
      "epoch 26\n",
      "Epoch: 27 \tTraining Loss: 0.546192\n",
      "epoch 27\n",
      "Epoch: 28 \tTraining Loss: 0.537094\n",
      "epoch 28\n",
      "Epoch: 29 \tTraining Loss: 0.524799\n",
      "epoch 29\n",
      "Epoch: 30 \tTraining Loss: 0.525464\n",
      "epoch 30\n",
      "Epoch: 31 \tTraining Loss: 0.511941\n",
      "epoch 31\n",
      "Epoch: 32 \tTraining Loss: 0.499042\n",
      "epoch 32\n",
      "Epoch: 33 \tTraining Loss: 0.482733\n",
      "epoch 33\n",
      "Epoch: 34 \tTraining Loss: 0.475189\n",
      "epoch 34\n",
      "Epoch: 35 \tTraining Loss: 0.464891\n",
      "epoch 35\n",
      "Epoch: 36 \tTraining Loss: 0.454873\n",
      "epoch 36\n",
      "Epoch: 37 \tTraining Loss: 0.449930\n",
      "epoch 37\n",
      "Epoch: 38 \tTraining Loss: 0.439655\n",
      "epoch 38\n",
      "Epoch: 39 \tTraining Loss: 0.433258\n",
      "epoch 39\n",
      "Epoch: 40 \tTraining Loss: 0.425072\n",
      "epoch 40\n",
      "Epoch: 41 \tTraining Loss: 0.421587\n",
      "epoch 41\n",
      "Epoch: 42 \tTraining Loss: 0.414485\n",
      "epoch 42\n",
      "Epoch: 43 \tTraining Loss: 0.408115\n",
      "epoch 43\n",
      "Epoch: 44 \tTraining Loss: 0.398340\n",
      "epoch 44\n",
      "Epoch: 45 \tTraining Loss: 0.392764\n",
      "epoch 45\n",
      "Epoch: 46 \tTraining Loss: 0.385030\n",
      "epoch 46\n",
      "Epoch: 47 \tTraining Loss: 0.374222\n",
      "epoch 47\n",
      "Epoch: 48 \tTraining Loss: 0.371016\n",
      "epoch 48\n",
      "Epoch: 49 \tTraining Loss: 0.361457\n",
      "epoch 49\n",
      "Epoch: 50 \tTraining Loss: 0.360136\n",
      "epoch 50\n",
      "Epoch: 51 \tTraining Loss: 0.353011\n",
      "epoch 51\n",
      "Epoch: 52 \tTraining Loss: 0.344631\n",
      "epoch 52\n",
      "Epoch: 53 \tTraining Loss: 0.345074\n",
      "epoch 53\n",
      "Epoch: 54 \tTraining Loss: 0.338935\n",
      "epoch 54\n",
      "Epoch: 55 \tTraining Loss: 0.334262\n",
      "epoch 55\n",
      "Epoch: 56 \tTraining Loss: 0.333332\n",
      "epoch 56\n",
      "Epoch: 57 \tTraining Loss: 0.324797\n",
      "epoch 57\n",
      "Epoch: 58 \tTraining Loss: 0.325080\n",
      "epoch 58\n",
      "Epoch: 59 \tTraining Loss: 0.326303\n",
      "epoch 59\n",
      "Epoch: 60 \tTraining Loss: 0.308609\n",
      "epoch 60\n",
      "Epoch: 61 \tTraining Loss: 0.304375\n",
      "epoch 61\n",
      "Epoch: 62 \tTraining Loss: 0.309870\n",
      "epoch 62\n",
      "Epoch: 63 \tTraining Loss: 0.298395\n",
      "epoch 63\n",
      "Epoch: 64 \tTraining Loss: 0.292893\n",
      "epoch 64\n",
      "Epoch: 65 \tTraining Loss: 0.296268\n",
      "epoch 65\n",
      "Epoch: 66 \tTraining Loss: 0.293841\n",
      "epoch 66\n",
      "Epoch: 67 \tTraining Loss: 0.287422\n",
      "epoch 67\n",
      "Epoch: 68 \tTraining Loss: 0.280857\n",
      "epoch 68\n",
      "Epoch: 69 \tTraining Loss: 0.280183\n",
      "epoch 69\n",
      "Epoch: 70 \tTraining Loss: 0.278991\n",
      "epoch 70\n",
      "Epoch: 71 \tTraining Loss: 0.273402\n",
      "epoch 71\n",
      "Epoch: 72 \tTraining Loss: 0.269177\n",
      "epoch 72\n",
      "Epoch: 73 \tTraining Loss: 0.274721\n",
      "epoch 73\n",
      "Epoch: 74 \tTraining Loss: 0.263950\n",
      "epoch 74\n",
      "Epoch: 75 \tTraining Loss: 0.258667\n",
      "epoch 75\n",
      "Epoch: 76 \tTraining Loss: 0.260476\n",
      "epoch 76\n",
      "Epoch: 77 \tTraining Loss: 0.260727\n",
      "epoch 77\n",
      "Epoch: 78 \tTraining Loss: 0.255037\n",
      "epoch 78\n",
      "Epoch: 79 \tTraining Loss: 0.251837\n",
      "epoch 79\n",
      "Epoch: 80 \tTraining Loss: 0.255660\n",
      "epoch 80\n",
      "Epoch: 81 \tTraining Loss: 0.248518\n",
      "epoch 81\n",
      "Epoch: 82 \tTraining Loss: 0.246857\n",
      "epoch 82\n",
      "Epoch: 83 \tTraining Loss: 0.243749\n",
      "epoch 83\n",
      "Epoch: 84 \tTraining Loss: 0.240709\n",
      "epoch 84\n",
      "Epoch: 85 \tTraining Loss: 0.243744\n",
      "epoch 85\n",
      "Epoch: 86 \tTraining Loss: 0.240420\n",
      "epoch 86\n",
      "Epoch: 87 \tTraining Loss: 0.236793\n",
      "epoch 87\n",
      "Epoch: 88 \tTraining Loss: 0.236878\n",
      "epoch 88\n",
      "Epoch: 89 \tTraining Loss: 0.228287\n",
      "epoch 89\n",
      "Epoch: 90 \tTraining Loss: 0.227192\n",
      "epoch 90\n",
      "Epoch: 91 \tTraining Loss: 0.232194\n",
      "epoch 91\n",
      "Epoch: 92 \tTraining Loss: 0.225498\n",
      "epoch 92\n",
      "Epoch: 93 \tTraining Loss: 0.226934\n",
      "epoch 93\n",
      "Epoch: 94 \tTraining Loss: 0.225597\n",
      "epoch 94\n",
      "Epoch: 95 \tTraining Loss: 0.218929\n",
      "epoch 95\n",
      "Epoch: 96 \tTraining Loss: 0.223861\n",
      "epoch 96\n",
      "Epoch: 97 \tTraining Loss: 0.214191\n",
      "epoch 97\n",
      "Epoch: 98 \tTraining Loss: 0.213230\n",
      "epoch 98\n",
      "Epoch: 99 \tTraining Loss: 0.214284\n",
      "epoch 99\n",
      "Epoch: 100 \tTraining Loss: 0.210892\n",
      "Running for Combination : leaky_relu 100\n",
      "epoch 0\n",
      "Epoch: 1 \tTraining Loss: 1.327574\n",
      "epoch 1\n",
      "Epoch: 2 \tTraining Loss: 1.242986\n",
      "epoch 2\n",
      "Epoch: 3 \tTraining Loss: 1.203876\n",
      "epoch 3\n",
      "Epoch: 4 \tTraining Loss: 1.166546\n",
      "epoch 4\n",
      "Epoch: 5 \tTraining Loss: 1.126425\n",
      "epoch 5\n",
      "Epoch: 6 \tTraining Loss: 1.083686\n",
      "epoch 6\n",
      "Epoch: 7 \tTraining Loss: 1.038374\n",
      "epoch 7\n",
      "Epoch: 8 \tTraining Loss: 0.992764\n",
      "epoch 8\n",
      "Epoch: 9 \tTraining Loss: 0.950206\n",
      "epoch 9\n",
      "Epoch: 10 \tTraining Loss: 0.908253\n",
      "epoch 10\n",
      "Epoch: 11 \tTraining Loss: 0.867803\n",
      "epoch 11\n",
      "Epoch: 12 \tTraining Loss: 0.831082\n",
      "epoch 12\n",
      "Epoch: 13 \tTraining Loss: 0.796101\n",
      "epoch 13\n",
      "Epoch: 14 \tTraining Loss: 0.762860\n",
      "epoch 14\n",
      "Epoch: 15 \tTraining Loss: 0.734650\n",
      "epoch 15\n",
      "Epoch: 16 \tTraining Loss: 0.706315\n",
      "epoch 16\n",
      "Epoch: 17 \tTraining Loss: 0.679735\n",
      "epoch 17\n",
      "Epoch: 18 \tTraining Loss: 0.656447\n",
      "epoch 18\n",
      "Epoch: 19 \tTraining Loss: 0.636303\n",
      "epoch 19\n",
      "Epoch: 20 \tTraining Loss: 0.619303\n",
      "epoch 20\n",
      "Epoch: 21 \tTraining Loss: 0.601296\n",
      "epoch 21\n",
      "Epoch: 22 \tTraining Loss: 0.587786\n",
      "epoch 22\n",
      "Epoch: 23 \tTraining Loss: 0.577033\n",
      "epoch 23\n",
      "Epoch: 24 \tTraining Loss: 0.569526\n",
      "epoch 24\n",
      "Epoch: 25 \tTraining Loss: 0.573028\n",
      "epoch 25\n",
      "Epoch: 26 \tTraining Loss: 0.564846\n",
      "epoch 26\n",
      "Epoch: 27 \tTraining Loss: 0.552707\n",
      "epoch 27\n",
      "Epoch: 28 \tTraining Loss: 0.532129\n",
      "epoch 28\n",
      "Epoch: 29 \tTraining Loss: 0.508802\n",
      "epoch 29\n",
      "Epoch: 30 \tTraining Loss: 0.491560\n",
      "epoch 30\n",
      "Epoch: 31 \tTraining Loss: 0.479813\n",
      "epoch 31\n",
      "Epoch: 32 \tTraining Loss: 0.469291\n",
      "epoch 32\n",
      "Epoch: 33 \tTraining Loss: 0.458004\n",
      "epoch 33\n",
      "Epoch: 34 \tTraining Loss: 0.449205\n",
      "epoch 34\n",
      "Epoch: 35 \tTraining Loss: 0.437835\n",
      "epoch 35\n",
      "Epoch: 36 \tTraining Loss: 0.431105\n",
      "epoch 36\n",
      "Epoch: 37 \tTraining Loss: 0.425384\n",
      "epoch 37\n",
      "Epoch: 38 \tTraining Loss: 0.422905\n",
      "epoch 38\n",
      "Epoch: 39 \tTraining Loss: 0.419957\n",
      "epoch 39\n",
      "Epoch: 40 \tTraining Loss: 0.415952\n",
      "epoch 40\n",
      "Epoch: 41 \tTraining Loss: 0.410676\n",
      "epoch 41\n",
      "Epoch: 42 \tTraining Loss: 0.399556\n",
      "epoch 42\n",
      "Epoch: 43 \tTraining Loss: 0.386519\n",
      "epoch 43\n",
      "Epoch: 44 \tTraining Loss: 0.376647\n",
      "epoch 44\n",
      "Epoch: 45 \tTraining Loss: 0.369311\n",
      "epoch 45\n",
      "Epoch: 46 \tTraining Loss: 0.365071\n",
      "epoch 46\n",
      "Epoch: 47 \tTraining Loss: 0.354756\n",
      "epoch 47\n",
      "Epoch: 48 \tTraining Loss: 0.347094\n",
      "epoch 48\n",
      "Epoch: 49 \tTraining Loss: 0.344714\n",
      "epoch 49\n",
      "Epoch: 50 \tTraining Loss: 0.343060\n",
      "epoch 50\n",
      "Epoch: 51 \tTraining Loss: 0.341168\n",
      "epoch 51\n",
      "Epoch: 52 \tTraining Loss: 0.341931\n",
      "epoch 52\n",
      "Epoch: 53 \tTraining Loss: 0.335603\n",
      "epoch 53\n",
      "Epoch: 54 \tTraining Loss: 0.330832\n",
      "epoch 54\n",
      "Epoch: 55 \tTraining Loss: 0.320282\n",
      "epoch 55\n",
      "Epoch: 56 \tTraining Loss: 0.316110\n",
      "epoch 56\n",
      "Epoch: 57 \tTraining Loss: 0.314691\n",
      "epoch 57\n",
      "Epoch: 58 \tTraining Loss: 0.309043\n",
      "epoch 58\n",
      "Epoch: 59 \tTraining Loss: 0.306265\n",
      "epoch 59\n",
      "Epoch: 60 \tTraining Loss: 0.306347\n",
      "epoch 60\n",
      "Epoch: 61 \tTraining Loss: 0.292889\n",
      "epoch 61\n",
      "Epoch: 62 \tTraining Loss: 0.295360\n",
      "epoch 62\n",
      "Epoch: 63 \tTraining Loss: 0.293288\n",
      "epoch 63\n",
      "Epoch: 64 \tTraining Loss: 0.285922\n",
      "epoch 64\n",
      "Epoch: 65 \tTraining Loss: 0.277761\n",
      "epoch 65\n",
      "Epoch: 66 \tTraining Loss: 0.278933\n",
      "epoch 66\n",
      "Epoch: 67 \tTraining Loss: 0.270572\n",
      "epoch 67\n",
      "Epoch: 68 \tTraining Loss: 0.275457\n",
      "epoch 68\n",
      "Epoch: 69 \tTraining Loss: 0.260776\n",
      "epoch 69\n",
      "Epoch: 70 \tTraining Loss: 0.261761\n",
      "epoch 70\n",
      "Epoch: 71 \tTraining Loss: 0.259810\n",
      "epoch 71\n",
      "Epoch: 72 \tTraining Loss: 0.255302\n",
      "epoch 72\n",
      "Epoch: 73 \tTraining Loss: 0.250702\n",
      "epoch 73\n",
      "Epoch: 74 \tTraining Loss: 0.251249\n",
      "epoch 74\n",
      "Epoch: 75 \tTraining Loss: 0.247757\n",
      "epoch 75\n",
      "Epoch: 76 \tTraining Loss: 0.244470\n",
      "epoch 76\n",
      "Epoch: 77 \tTraining Loss: 0.245888\n",
      "epoch 77\n",
      "Epoch: 78 \tTraining Loss: 0.242872\n",
      "epoch 78\n",
      "Epoch: 79 \tTraining Loss: 0.234365\n",
      "epoch 79\n",
      "Epoch: 80 \tTraining Loss: 0.234257\n",
      "epoch 80\n",
      "Epoch: 81 \tTraining Loss: 0.235082\n",
      "epoch 81\n",
      "Epoch: 82 \tTraining Loss: 0.231429\n",
      "epoch 82\n",
      "Epoch: 83 \tTraining Loss: 0.232612\n",
      "epoch 83\n",
      "Epoch: 84 \tTraining Loss: 0.225524\n",
      "epoch 84\n",
      "Epoch: 85 \tTraining Loss: 0.225781\n",
      "epoch 85\n",
      "Epoch: 86 \tTraining Loss: 0.218840\n",
      "epoch 86\n",
      "Epoch: 87 \tTraining Loss: 0.226094\n",
      "epoch 87\n",
      "Epoch: 88 \tTraining Loss: 0.216815\n",
      "epoch 88\n",
      "Epoch: 89 \tTraining Loss: 0.219496\n",
      "epoch 89\n",
      "Epoch: 90 \tTraining Loss: 0.218355\n",
      "epoch 90\n",
      "Epoch: 91 \tTraining Loss: 0.213372\n",
      "epoch 91\n",
      "Epoch: 92 \tTraining Loss: 0.212919\n",
      "epoch 92\n",
      "Epoch: 93 \tTraining Loss: 0.209016\n",
      "epoch 93\n",
      "Epoch: 94 \tTraining Loss: 0.208089\n",
      "epoch 94\n",
      "Epoch: 95 \tTraining Loss: 0.206682\n",
      "epoch 95\n",
      "Epoch: 96 \tTraining Loss: 0.204618\n",
      "epoch 96\n",
      "Epoch: 97 \tTraining Loss: 0.206794\n",
      "epoch 97\n",
      "Epoch: 98 \tTraining Loss: 0.205306\n",
      "epoch 98\n",
      "Epoch: 99 \tTraining Loss: 0.199167\n",
      "epoch 99\n",
      "Epoch: 100 \tTraining Loss: 0.202057\n"
     ]
    }
   ],
   "source": [
    "train_data_f10, test_data_f10 = review_dataset(X_train_vect_concat, y_train), review_dataset(X_test_vect_concat, y_test)\n",
    "train_dataloader_f10 = DataLoader(train_data_f10, batch_size=64)\n",
    "test_dataloader_f10 = DataLoader(test_data_f10, batch_size=1)\n",
    "\n",
    "N_EPOCHS = [100]\n",
    "activation_funcs = [\"relu\", \"leaky_relu\"]\n",
    "from itertools import product\n",
    "output_df2 = pd.DataFrame(product(activation_funcs, N_EPOCHS), columns = ['Activation_Function', 'N_Epochs'])\n",
    "test_accuracies = []\n",
    "for activation_func in activation_funcs:\n",
    "    for i, epochs in enumerate(N_EPOCHS):\n",
    "        print(\"Running for Combination :\", activation_func, epochs)\n",
    "        INPUT_DIM = 3000\n",
    "        OUTPUT_DIM = 5\n",
    "\n",
    "        model = MLP_mod(INPUT_DIM, OUTPUT_DIM, activation_func)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "\n",
    "        updated_model = train(model, train_dataloader_f10, optimizer, criterion, device, epochs)\n",
    "  \n",
    "        prediction_list = []\n",
    "        for i, batch in enumerate(test_dataloader_f10):   \n",
    "            outputs = updated_model(batch[0])\n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            prediction_list.append(predicted)    \n",
    "            \n",
    "        predictions = np.array(prediction_list)\n",
    "        tp=0\n",
    "        tp += (predictions == y_test).sum()\n",
    "        accuracy = 100 * tp / len(predictions)\n",
    "        test_accuracies.append(accuracy)\n",
    "\n",
    "output_df2['Test_Accuracy'] = test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activation_Function</th>\n",
       "      <th>N_Epochs</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>36.987885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leaky_relu</td>\n",
       "      <td>100</td>\n",
       "      <td>36.894273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Activation_Function  N_Epochs  Test_Accuracy\n",
       "0                relu       100      36.987885\n",
       "1          leaky_relu       100      36.894273"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "X_train_vect_rnn = []\n",
    "for i in X_train_vect:\n",
    "    subset_list = []\n",
    "    for v in range(20):\n",
    "        try:\n",
    "            if i[v].size:\n",
    "                subset_list.append(i[v])\n",
    "        except:\n",
    "            subset_list.append(np.zeros(300, dtype=float))\n",
    "    X_train_vect_rnn.append(np.array(subset_list))\n",
    "X_train_vect_rnn = np.array(X_train_vect_rnn)\n",
    "\n",
    "X_test_vect_rnn = []\n",
    "for i in X_test_vect:\n",
    "    subset_list = []\n",
    "    for v in range(20):\n",
    "        try:\n",
    "            if i[v].size:\n",
    "                subset_list.append(i[v])\n",
    "        except:\n",
    "            subset_list.append(np.zeros(300, dtype=float))\n",
    "    X_test_vect_rnn.append(np.array(subset_list))\n",
    "X_test_vect_rnn = np.array(X_test_vect_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72619, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "print((X_train_vect_rnn).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model\n",
    "\n",
    "n_categories = 5\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input = input.reshape(1, -1)\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "n_words = 300\n",
    "n_hidden = 20\n",
    "\n",
    "rnn = RNN(n_words, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "batch_size = 64\n",
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "def train(rnn, iterator, optimizer, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    rnn = rnn.to(device)\n",
    "    rnn.train()\n",
    "\n",
    "    for x, y in iterator:\n",
    "        if x.shape[0] != batch_size:\n",
    "                continue\n",
    "        hidden = torch.zeros(batch_size, n_hidden,requires_grad=False)\n",
    "        hidden = hidden.to(device)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        rnn.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(x.shape[1]):\n",
    "            output, hidden = rnn(x[:, i,:], hidden)\n",
    "            l = criterion(output, y)\n",
    "            loss += l\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "\n",
    "        acc = calculate_accuracy(output, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(rnn, iterator, criterion, device):\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # rnn = rnn.to(device)\n",
    "    rnn.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "        for x, y in iterator:\n",
    "            # if x.shape[0] != batch_size:\n",
    "            #     continue\n",
    "            hidden = torch.zeros(1, n_hidden,requires_grad=False)\n",
    "            hidden = hidden.to(device)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            for i in range(x.shape[1]):\n",
    "                output, hidden = rnn(x[:, i, :], hidden)\n",
    "\n",
    "            # loss = criterion(output, y)\n",
    "\n",
    "            acc = calculate_accuracy(output, y)\n",
    "\n",
    "            # epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e305fb311cc4fbaa38df1ceca736579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1.366 | Train Acc: 36.69%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 1.303 | Train Acc: 40.61%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 1.298 | Train Acc: 41.00%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 1.274 | Train Acc: 42.93%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 1.234 | Train Acc: 45.57%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 1.230 | Train Acc: 46.18%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 1.225 | Train Acc: 46.29%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 1.223 | Train Acc: 46.57%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 1.222 | Train Acc: 46.51%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 1.220 | Train Acc: 46.65%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 1.217 | Train Acc: 46.60%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 1.218 | Train Acc: 46.69%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 1.216 | Train Acc: 46.97%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 1.214 | Train Acc: 46.94%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 1.213 | Train Acc: 47.04%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 1.212 | Train Acc: 47.23%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 1.209 | Train Acc: 47.33%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 1.208 | Train Acc: 47.37%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 1.206 | Train Acc: 47.61%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 1.205 | Train Acc: 47.64%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 1.206 | Train Acc: 47.45%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 1.204 | Train Acc: 47.64%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 1.204 | Train Acc: 47.64%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 1.202 | Train Acc: 47.63%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 1.202 | Train Acc: 47.79%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 1.200 | Train Acc: 47.87%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 1.200 | Train Acc: 47.76%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 1.200 | Train Acc: 47.81%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 1.198 | Train Acc: 48.18%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 1.197 | Train Acc: 47.94%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 1.199 | Train Acc: 48.15%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 1.196 | Train Acc: 48.11%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 1.196 | Train Acc: 48.01%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 1.195 | Train Acc: 48.16%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 1.194 | Train Acc: 48.16%\n",
      "Epoch: 36\n",
      "\tTrain Loss: 1.194 | Train Acc: 48.13%\n",
      "Epoch: 37\n",
      "\tTrain Loss: 1.194 | Train Acc: 48.20%\n",
      "Epoch: 38\n",
      "\tTrain Loss: 1.191 | Train Acc: 48.42%\n",
      "Epoch: 39\n",
      "\tTrain Loss: 1.193 | Train Acc: 48.31%\n",
      "Epoch: 40\n",
      "\tTrain Loss: 1.191 | Train Acc: 48.43%\n",
      "Epoch: 41\n",
      "\tTrain Loss: 1.192 | Train Acc: 48.34%\n",
      "Epoch: 42\n",
      "\tTrain Loss: 1.189 | Train Acc: 48.45%\n",
      "Epoch: 43\n",
      "\tTrain Loss: 1.189 | Train Acc: 48.48%\n",
      "Epoch: 44\n",
      "\tTrain Loss: 1.188 | Train Acc: 48.36%\n",
      "Epoch: 45\n",
      "\tTrain Loss: 1.186 | Train Acc: 48.61%\n",
      "Epoch: 46\n",
      "\tTrain Loss: 1.185 | Train Acc: 48.72%\n",
      "Epoch: 47\n",
      "\tTrain Loss: 1.185 | Train Acc: 48.66%\n",
      "Epoch: 48\n",
      "\tTrain Loss: 1.184 | Train Acc: 48.83%\n",
      "Epoch: 49\n",
      "\tTrain Loss: 1.184 | Train Acc: 48.73%\n",
      "Epoch: 50\n",
      "\tTrain Loss: 1.183 | Train Acc: 48.80%\n",
      "Epoch: 51\n",
      "\tTrain Loss: 1.182 | Train Acc: 48.79%\n",
      "Epoch: 52\n",
      "\tTrain Loss: 1.183 | Train Acc: 48.86%\n",
      "Epoch: 53\n",
      "\tTrain Loss: 1.181 | Train Acc: 48.84%\n",
      "Epoch: 54\n",
      "\tTrain Loss: 1.181 | Train Acc: 48.96%\n",
      "Epoch: 55\n",
      "\tTrain Loss: 1.181 | Train Acc: 48.94%\n",
      "Epoch: 56\n",
      "\tTrain Loss: 1.180 | Train Acc: 49.04%\n",
      "Epoch: 57\n",
      "\tTrain Loss: 1.179 | Train Acc: 49.10%\n",
      "Epoch: 58\n",
      "\tTrain Loss: 1.179 | Train Acc: 49.01%\n",
      "Epoch: 59\n",
      "\tTrain Loss: 1.178 | Train Acc: 48.95%\n",
      "Epoch: 60\n",
      "\tTrain Loss: 1.178 | Train Acc: 48.95%\n",
      "Epoch: 61\n",
      "\tTrain Loss: 1.177 | Train Acc: 49.13%\n",
      "Epoch: 62\n",
      "\tTrain Loss: 1.177 | Train Acc: 49.23%\n",
      "Epoch: 63\n",
      "\tTrain Loss: 1.177 | Train Acc: 49.21%\n",
      "Epoch: 64\n",
      "\tTrain Loss: 1.176 | Train Acc: 49.00%\n",
      "Epoch: 65\n",
      "\tTrain Loss: 1.176 | Train Acc: 49.07%\n",
      "Epoch: 66\n",
      "\tTrain Loss: 1.176 | Train Acc: 49.15%\n",
      "Epoch: 67\n",
      "\tTrain Loss: 1.175 | Train Acc: 49.17%\n",
      "Epoch: 68\n",
      "\tTrain Loss: 1.175 | Train Acc: 49.28%\n",
      "Epoch: 69\n",
      "\tTrain Loss: 1.174 | Train Acc: 49.27%\n",
      "Epoch: 70\n",
      "\tTrain Loss: 1.175 | Train Acc: 49.05%\n",
      "Epoch: 71\n",
      "\tTrain Loss: 1.174 | Train Acc: 49.18%\n",
      "Epoch: 72\n",
      "\tTrain Loss: 1.174 | Train Acc: 49.21%\n",
      "Epoch: 73\n",
      "\tTrain Loss: 1.174 | Train Acc: 49.14%\n",
      "Epoch: 74\n",
      "\tTrain Loss: 1.173 | Train Acc: 49.30%\n",
      "Epoch: 75\n",
      "\tTrain Loss: 1.174 | Train Acc: 49.24%\n",
      "Epoch: 76\n",
      "\tTrain Loss: 1.173 | Train Acc: 49.33%\n",
      "Epoch: 77\n",
      "\tTrain Loss: 1.173 | Train Acc: 49.43%\n",
      "Epoch: 78\n",
      "\tTrain Loss: 1.173 | Train Acc: 49.40%\n",
      "Epoch: 79\n",
      "\tTrain Loss: 1.172 | Train Acc: 49.40%\n",
      "Epoch: 80\n",
      "\tTrain Loss: 1.172 | Train Acc: 49.34%\n",
      "Epoch: 81\n",
      "\tTrain Loss: 1.173 | Train Acc: 49.38%\n",
      "Epoch: 82\n",
      "\tTrain Loss: 1.172 | Train Acc: 49.32%\n",
      "Epoch: 83\n",
      "\tTrain Loss: 1.172 | Train Acc: 49.45%\n",
      "Epoch: 84\n",
      "\tTrain Loss: 1.171 | Train Acc: 49.38%\n",
      "Epoch: 85\n",
      "\tTrain Loss: 1.171 | Train Acc: 49.51%\n",
      "Epoch: 86\n",
      "\tTrain Loss: 1.171 | Train Acc: 49.39%\n",
      "Epoch: 87\n",
      "\tTrain Loss: 1.170 | Train Acc: 49.43%\n",
      "Epoch: 88\n",
      "\tTrain Loss: 1.170 | Train Acc: 49.41%\n",
      "Epoch: 89\n",
      "\tTrain Loss: 1.171 | Train Acc: 49.44%\n",
      "Epoch: 90\n",
      "\tTrain Loss: 1.170 | Train Acc: 49.41%\n",
      "Epoch: 91\n",
      "\tTrain Loss: 1.170 | Train Acc: 49.47%\n",
      "Epoch: 92\n",
      "\tTrain Loss: 1.169 | Train Acc: 49.47%\n",
      "Epoch: 93\n",
      "\tTrain Loss: 1.170 | Train Acc: 49.48%\n",
      "Epoch: 94\n",
      "\tTrain Loss: 1.169 | Train Acc: 49.50%\n",
      "Epoch: 95\n",
      "\tTrain Loss: 1.170 | Train Acc: 49.52%\n",
      "Epoch: 96\n",
      "\tTrain Loss: 1.169 | Train Acc: 49.46%\n",
      "Epoch: 97\n",
      "\tTrain Loss: 1.170 | Train Acc: 49.53%\n",
      "Epoch: 98\n",
      "\tTrain Loss: 1.168 | Train Acc: 49.67%\n",
      "Epoch: 99\n",
      "\tTrain Loss: 1.168 | Train Acc: 49.57%\n",
      "Epoch: 100\n",
      "\tTrain Loss: 1.168 | Train Acc: 49.53%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# optimizer = optim.SGD(rnn.parameters(), lr = 0.001)\n",
    "optimizer = optim.Adam(rnn.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "train_data_rnn, test_data_rnn = review_dataset(X_train_vect_rnn, y_train), review_dataset(X_test_vect_rnn, y_test)\n",
    "train_dataloader_rnn = DataLoader(train_data_rnn, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(rnn, train_dataloader_rnn, optimizer, criterion, device)\n",
    "    # test_loss, test_acc = evaluate(rnn, test_dataloader, criterion, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    # print(f'\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_rnn = DataLoader(test_data_rnn, batch_size=1)\n",
    "test_acc = evaluate(rnn, test_dataloader_rnn, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4422473147893142"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.layer1 = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "        self.layer2 = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "        \n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \n",
    "        inp = self.layer1(input)\n",
    "        h = self.layer2(hidden)\n",
    "\n",
    "        inp_reset, inp_upd, inp_new = inp.chunk(3, 1)\n",
    "        hidden_reset, hidden_upd, hidden_new = h.chunk(3, 1)\n",
    "\n",
    "        reset_gate = torch.sigmoid(inp_reset + hidden_reset)\n",
    "        update_gate = torch.sigmoid(inp_upd + hidden_upd)\n",
    "        new_gate = torch.tanh(inp_new + (reset_gate * hidden_new))\n",
    "\n",
    "        out = update_gate * hidden + (1 - update_gate) * new_gate\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, drop_prob=0):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # self.n_layers = n_layers\n",
    "        \n",
    "        # self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.gru = GRU(input_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(h))\n",
    "        out = self.softmax(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUNet(\n",
       "  (gru): GRU(\n",
       "    (layer1): Linear(in_features=300, out_features=60, bias=True)\n",
       "    (layer2): Linear(in_features=20, out_features=60, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru = GRUNet(300, 20, 5)\n",
    "model_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29e8f3506a542f3aa355a4530161b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 1.352750 | Train Acc: 37.90%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 1.178736 | Train Acc: 47.89%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 1.137225 | Train Acc: 49.67%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 1.114993 | Train Acc: 50.74%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 1.100405 | Train Acc: 51.41%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 1.089523 | Train Acc: 51.96%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 1.080737 | Train Acc: 52.37%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 1.072892 | Train Acc: 52.77%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 1.065436 | Train Acc: 53.10%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 1.059081 | Train Acc: 53.23%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 1.053607 | Train Acc: 53.58%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 1.049011 | Train Acc: 53.81%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 1.043446 | Train Acc: 53.95%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 1.040088 | Train Acc: 54.19%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 1.035862 | Train Acc: 54.32%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 1.031963 | Train Acc: 54.44%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 1.026862 | Train Acc: 55.05%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 1.023113 | Train Acc: 55.06%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 1.021010 | Train Acc: 55.07%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 1.016056 | Train Acc: 55.46%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 1.012921 | Train Acc: 55.52%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 1.010949 | Train Acc: 55.66%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 1.006312 | Train Acc: 55.89%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 1.004338 | Train Acc: 55.95%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 1.000639 | Train Acc: 56.11%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.997652 | Train Acc: 56.04%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.994694 | Train Acc: 56.42%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.992324 | Train Acc: 56.64%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.989111 | Train Acc: 56.69%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.986994 | Train Acc: 56.71%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.984861 | Train Acc: 56.92%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.981449 | Train Acc: 56.92%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.978925 | Train Acc: 57.10%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.976858 | Train Acc: 57.12%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.974596 | Train Acc: 57.43%\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.973072 | Train Acc: 57.46%\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.969620 | Train Acc: 57.70%\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.967111 | Train Acc: 57.79%\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.965129 | Train Acc: 57.82%\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.962290 | Train Acc: 58.00%\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.960941 | Train Acc: 57.96%\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.959176 | Train Acc: 58.11%\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.957041 | Train Acc: 58.08%\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.953862 | Train Acc: 58.40%\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.952931 | Train Acc: 58.31%\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.950227 | Train Acc: 58.47%\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.947819 | Train Acc: 58.65%\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.946189 | Train Acc: 58.78%\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.944787 | Train Acc: 58.78%\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.941276 | Train Acc: 58.91%\n",
      "Epoch: 51\n",
      "\tTrain Loss: 0.939783 | Train Acc: 59.12%\n",
      "Epoch: 52\n",
      "\tTrain Loss: 0.939326 | Train Acc: 59.16%\n",
      "Epoch: 53\n",
      "\tTrain Loss: 0.937401 | Train Acc: 59.13%\n",
      "Epoch: 54\n",
      "\tTrain Loss: 0.934413 | Train Acc: 59.44%\n",
      "Epoch: 55\n",
      "\tTrain Loss: 0.932947 | Train Acc: 59.41%\n",
      "Epoch: 56\n",
      "\tTrain Loss: 0.930644 | Train Acc: 59.63%\n",
      "Epoch: 57\n",
      "\tTrain Loss: 0.930367 | Train Acc: 59.57%\n",
      "Epoch: 58\n",
      "\tTrain Loss: 0.927073 | Train Acc: 59.68%\n",
      "Epoch: 59\n",
      "\tTrain Loss: 0.926524 | Train Acc: 59.83%\n",
      "Epoch: 60\n",
      "\tTrain Loss: 0.924084 | Train Acc: 59.86%\n",
      "Epoch: 61\n",
      "\tTrain Loss: 0.922599 | Train Acc: 60.10%\n",
      "Epoch: 62\n",
      "\tTrain Loss: 0.921399 | Train Acc: 59.90%\n",
      "Epoch: 63\n",
      "\tTrain Loss: 0.919833 | Train Acc: 59.95%\n",
      "Epoch: 64\n",
      "\tTrain Loss: 0.917915 | Train Acc: 60.18%\n",
      "Epoch: 65\n",
      "\tTrain Loss: 0.916252 | Train Acc: 60.28%\n",
      "Epoch: 66\n",
      "\tTrain Loss: 0.914197 | Train Acc: 60.39%\n",
      "Epoch: 67\n",
      "\tTrain Loss: 0.912931 | Train Acc: 60.39%\n",
      "Epoch: 68\n",
      "\tTrain Loss: 0.912227 | Train Acc: 60.65%\n",
      "Epoch: 69\n",
      "\tTrain Loss: 0.909261 | Train Acc: 60.55%\n",
      "Epoch: 70\n",
      "\tTrain Loss: 0.908059 | Train Acc: 60.64%\n",
      "Epoch: 71\n",
      "\tTrain Loss: 0.906541 | Train Acc: 60.74%\n",
      "Epoch: 72\n",
      "\tTrain Loss: 0.905724 | Train Acc: 60.94%\n",
      "Epoch: 73\n",
      "\tTrain Loss: 0.903591 | Train Acc: 60.94%\n",
      "Epoch: 74\n",
      "\tTrain Loss: 0.901927 | Train Acc: 61.16%\n",
      "Epoch: 75\n",
      "\tTrain Loss: 0.901409 | Train Acc: 61.03%\n",
      "Epoch: 76\n",
      "\tTrain Loss: 0.900132 | Train Acc: 61.14%\n",
      "Epoch: 77\n",
      "\tTrain Loss: 0.898941 | Train Acc: 61.21%\n",
      "Epoch: 78\n",
      "\tTrain Loss: 0.895784 | Train Acc: 61.32%\n",
      "Epoch: 79\n",
      "\tTrain Loss: 0.895327 | Train Acc: 61.41%\n",
      "Epoch: 80\n",
      "\tTrain Loss: 0.894939 | Train Acc: 61.44%\n",
      "Epoch: 81\n",
      "\tTrain Loss: 0.892696 | Train Acc: 61.56%\n",
      "Epoch: 82\n",
      "\tTrain Loss: 0.891341 | Train Acc: 61.53%\n",
      "Epoch: 83\n",
      "\tTrain Loss: 0.889870 | Train Acc: 61.67%\n",
      "Epoch: 84\n",
      "\tTrain Loss: 0.889197 | Train Acc: 61.75%\n",
      "Epoch: 85\n",
      "\tTrain Loss: 0.887928 | Train Acc: 61.75%\n",
      "Epoch: 86\n",
      "\tTrain Loss: 0.886542 | Train Acc: 61.76%\n",
      "Epoch: 87\n",
      "\tTrain Loss: 0.885779 | Train Acc: 61.89%\n",
      "Epoch: 88\n",
      "\tTrain Loss: 0.884797 | Train Acc: 61.92%\n",
      "Epoch: 89\n",
      "\tTrain Loss: 0.883455 | Train Acc: 62.12%\n",
      "Epoch: 90\n",
      "\tTrain Loss: 0.882182 | Train Acc: 62.11%\n",
      "Epoch: 91\n",
      "\tTrain Loss: 0.880873 | Train Acc: 62.07%\n",
      "Epoch: 92\n",
      "\tTrain Loss: 0.880094 | Train Acc: 62.14%\n",
      "Epoch: 93\n",
      "\tTrain Loss: 0.878046 | Train Acc: 62.24%\n",
      "Epoch: 94\n",
      "\tTrain Loss: 0.876877 | Train Acc: 62.29%\n",
      "Epoch: 95\n",
      "\tTrain Loss: 0.876115 | Train Acc: 62.37%\n",
      "Epoch: 96\n",
      "\tTrain Loss: 0.875149 | Train Acc: 62.51%\n",
      "Epoch: 97\n",
      "\tTrain Loss: 0.873920 | Train Acc: 62.44%\n",
      "Epoch: 98\n",
      "\tTrain Loss: 0.872604 | Train Acc: 62.51%\n",
      "Epoch: 99\n",
      "\tTrain Loss: 0.873214 | Train Acc: 62.49%\n",
      "Epoch: 100\n",
      "\tTrain Loss: 0.871301 | Train Acc: 62.52%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# optimizer = optim.SGD(rnn.parameters(), lr = 0.001)\n",
    "optimizer = optim.Adam(model_gru.parameters())\n",
    "criterion = nn.NLLLoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "n_words = 300\n",
    "n_hidden = 20\n",
    "\n",
    "# train_data_rnn, test_data_rnn = review_dataset(X_train_vect_rnn, y_train), review_dataset(X_test_vect_rnn, y_test)\n",
    "# train_dataloader_rnn = DataLoader(train_data_rnn, batch_size=64, shuffle=True)\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "train_data_rnn, test_data_rnn = review_dataset(X_train_vect_rnn, y_train), review_dataset(X_test_vect_rnn, y_test)\n",
    "train_dataloader_rnn = DataLoader(train_data_rnn, batch_size=64, shuffle=True)\n",
    "for epoch in trange(EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model_gru, train_dataloader_rnn, optimizer, criterion, device)\n",
    "    # test_loss, test_acc = evaluate(rnn, test_dataloader, criterion, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.6f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    # print(f'\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader_rnn = DataLoader(test_data_rnn, batch_size=1)\n",
    "test_acc = evaluate(model_gru, test_dataloader_rnn, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4845814977973568"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
